分类问题用什么模型

1. 决策树分类(Decision Tree):
    - 概括： 以树状结构构建分类模型， 通过将数据集不断拆分为更小的子集来使decision tree不断生长， 最终长成具有decision nodes和leaf nodes的数。用leaf来表示表示最终的分类结果。
            特征的分类能力： 用熵entropy来表示。Which means 随机变量不确定性的度量。 不确定性越大， entropy越大。
                如何计算entropy： 暂略
            信息增益 information gain： 得知 特征X 的信息使得 分类Y 的不确定性减少 (entropy decrease) 的程度。
                 Gain(A) = H(D) - H(D|A)
            
            决策树归纳算法： 都是greedy algorithm, top down approach.
            ID3 (Iterative Dichotomiser 3): 从root开始， 对internal node计算所有可能的information gain， 选择gain最大的特征作为这个internal node的特征。
                                                      使用上述方法Iteratively， 构建decision tree， 直到所有information gain都变得很小。
                                                     
            C4.5 Algorithm: 使用信息增益比来进行特征选择。
            CART Algorithm: Classification and regression tree. 假设决策树是一个二叉树， 通过递归的二分每个特征，将特征划分为有限个单元，并在这些单元上确定预测的概率分布。
            
            决策树的剪枝 prunning:
            当数据分的过细时，虽然在training set上表现比较好，但是在别的新数据上可能会表现不佳，出现过度拟合的状况。
            Solution 1: When entropy < threshold: stop creating branches
            Solution 2: After creating the whole decision tree, remove extra nodes. (Prunning).
            
    - 总结： decision tree易于理解，直观。可直接处理 categorical data， 甚至可以处理data with missing entries.
            However, decision tree cannot handle continuous varaiable, 不好处理错综复杂的组合。



2. 随机森林 Random Forest, belongs to (Bagging) bootstrap aggregation
    - 概括： 随机森林由很多independent的决策树构成。进行分类任务时，新的输入样本，让森林中的每一棵决策树进行分类并返回分类结果， 哪一种分类结果最多，随机森林就会把这个结果当作最终的结果。
            Procedure: 
            1. 一个样本容量为N的样本，有放回的抽取N次，每次抽取一个，最终形成N个样本，用这N个样本作为决策树root的样本来训练决策树。
            2. 当每个样本有M个特征时，在决策树每个internal node进行分裂时，随机从这M个特征中取出一个m大小的subset，（m << M. 然后从这m个特征中采取某种策略（information gain）来选择一个特征作为该node的分裂特征。
            3. 决策树形成过程中的每个node分裂都按照2的步骤。
            重复1-3，即可生成随机森林。
     
     - 总结：
     优点： 可以生成特征很多的数据； 可以判断不同特征的重要程度； 可以判断不同特征间的相互影响。
            不容易过度拟合，训练速度快，实现简单。
     缺点： 在某些noise较大的分类或者回归问题上会过度拟合。
            对于有不同取值的特征的数据，取值划分较多的特征会对随机森林产生更大的影响，所以随机森林在这种数据上对不同特征的weight不可信。
            
3. Logistic Regression: 主要解决二分类 （binary） 问题 yes or no question， 用来表示某件事情发生的可能性。
	优点：实现简单，计算量小，易于理解和实现
		可用L2正则化来解决多重线性的问题
	缺点：特征空间很大时，逻辑回归的性能不是很好。
		容易欠拟合，准确率不高
		只能处理二分类问题，且必须线性可分。（要求因变量是离散的变量）

4. Linear Regression: 主要用于回归。回归用于预测，通过历史的的数据来预测未来的结果。
利用变量之间的线性关系，来建立有效的模型，预测未来变量的结果。

	Y = w’ x + e, e是误差服从均值为0的正态分布。

	优点：建模速度快，运行速度快，可以给出对于变量的理解和解释
	缺点：拟合性差，需要先判断变量之间是否是线性关系。
	scipy.polyfit() or numpy.polyfit()
	Stats.linregress()
	sklearn.linear_model.LinearRegression()
	
	线性回归要求因变量是连续的变量，且自变量与因变量之间要成线性关系。



       
     
     
            
            
            
                 
           
